{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import utils, datasets\n",
    "from torchvision.datasets import Cityscapes, MNIST\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# for experiment tracking\n",
    "import neptune\n",
    "import neptune_pytorch\n",
    "from getpass import getpass\n",
    "\n",
    "state_dict_only =  1\n",
    "whole_model = 2\n",
    "both = 0\n",
    "\n",
    "# check gpu availablity\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/zkim/drivablearea/semantic-segmentation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import parameters \n",
    "import yaml\n",
    "\n",
    "cfg = yaml.safe_load(open(\"./configs/cityscapes.yaml\"))\n",
    "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "train_cfg, eval_cfg, test_cfg = cfg['TRAIN'], cfg['EVAL'], cfg['TEST']\n",
    "#model = eval(model_cfg['NAME'])(model_cfg['BACKBONE'], trainset.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_size = cfg['TRAIN']['IMAGE_SIZE']\n",
    "batch_size = cfg['TRAIN']['BATCH_SIZE']\n",
    "epoch = cfg['TRAIN']['EPOCHS']\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_401037/3393632705.py:3: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/zkim/drivable-area/e/DRIV-1\n"
     ]
    }
   ],
   "source": [
    "#experiment tracking tool \n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"zkim/drivable-area\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDRiNjgzYi1lNGRiLTQzY2YtYjUwMy1mYTFjOWI0YzM1NTIifQ==\",\n",
    ") \n",
    "\n",
    "parameters = {\n",
    "    \"lr\": 1e-2,\n",
    "    \"bs\": 128,\n",
    "    \"input_sz\": 32 * 32 * 3,\n",
    "    \"n_classes\": 10,\n",
    "    \"model_filename\": \"basemodel\",\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"epochs\": 2,\n",
    "}\n",
    "# if you need to use it\n",
    "# model = Model(\n",
    "#     parameters[\"input_sz\"], parameters[\"input_sz\"], parameters[\"n_classes\"]\n",
    "# ).to(parameters[\"device\"])\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=parameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/CityScapes'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cfg['ROOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torchvision.datasets to process data\n",
    "\n",
    "\n",
    "dataset_path = dataset_cfg['ROOT']\n",
    "\n",
    "transforms_train_img = transforms.Compose([\n",
    "    #transforms.PILToTensor(),\n",
    "    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # transforms.ColorJitter(brightness=0.0, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    # transforms.RandomAdjustSharpness(sharpness_factor=0.1, p=0.5),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomGrayscale(p=0.5),\n",
    "    # transforms.RandomRotation(degrees=10, p=0.3, seg_fill=seg_fill),\n",
    "    # transforms.RandomResizedCrop(size, scale=(0.5, 2.0), seg_fill=seg_fill),\n",
    "    transforms.Resize(size=(256,512)), # default size: [2048 w, 1024 h]\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "transforms_train_label = transforms.Compose([\n",
    "    # transforms.ColorJitter(brightness=0.0, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    # transforms.RandomAdjustSharpness(sharpness_factor=0.1, p=0.5),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomGrayscale(p=0.5),\n",
    "    # transforms.RandomRotation(degrees=10, p=0.3, seg_fill=seg_fill),\n",
    "    # transforms.RandomResizedCrop(size, scale=(0.5, 2.0), seg_fill=seg_fill),\n",
    "    transforms.Resize(size=(256,512)), # default size: [2048 w, 1024 h]\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms_val_test = transforms.Compose([\n",
    "    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.Resize(size=(256,512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# cityscapes_train_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=transforms.ToTensor(), target_transform=transforms.ToTensor())\n",
    "cityscapes_train_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=transforms_train_img, target_transform=transforms_train_label)\n",
    "cityscapes_val_dataset = Cityscapes(dataset_path, split='val', mode='fine', target_type='semantic', transform=transforms_val_test, target_transform=transforms_val_test)\n",
    "cityscapes_test_dataset = Cityscapes(dataset_path, split='test', mode='fine', target_type='semantic', transform=transforms_val_test, target_transform=transforms_val_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cityscapes_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, smnt = cityscapes_test_dataset[1]\n",
    "type(smnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 256]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.functional.get_image_size(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader \n",
    "\n",
    "#from semseg.augmentations import *\n",
    "#from semseg.datasets import CamVid\n",
    "\n",
    "train_dataloader = DataLoader(cityscapes_train_dataset, batch_size=batch_size, shuffle=\"False\", num_workers=4)\n",
    "val_dataloader = DataLoader(cityscapes_val_dataset, batch_size=batch_size, shuffle=\"False\", num_workers=4)\n",
    "test_dataloader = DataLoader(cityscapes_test_dataset, batch_size=batch_size, shuffle=\"False\", num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5f668457b0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone Names    Variants\n",
      "----------------  ------------------------------------\n",
      "ResNet            ['18', '34', '50', '101', '152']\n",
      "ResNetD           ['18', '50', '101']\n",
      "MicroNet          ['M1', 'M2', 'M3']\n",
      "MobileNetV2       ['1.0']\n",
      "MobileNetV3       ['S', 'L']\n",
      "MiT               ['B0', 'B1', 'B2', 'B3', 'B4', 'B5']\n",
      "PVTv2             ['B1', 'B2', 'B3', 'B4', 'B5']\n",
      "ResT              ['S', 'B', 'L']\n",
      "PoolFormer        ['S24', 'S36', 'M36']\n",
      "ConvNeXt          ['T', 'S', 'B']\n"
     ]
    }
   ],
   "source": [
    "from semseg import show_models, show_backbones, show_heads, show_datasets\n",
    "\n",
    "show_backbones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCNN(BaseModel):\n",
    "#     def __init__(self, backbone: str = 'ResNet-50', num_classes: int = 19): backbone in string\n",
    "#         super().__init__(backbone, num_classes)\n",
    "#         self.decode_head = UPerHead(self.backbone.channels, 256, num_classes) # choose head\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def forward(self, x: Tensor) -> Tensor:\n",
    "#         y = self.backbone(x)\n",
    "#         y = self.decode_head(y)   # 4x reduction in image size\n",
    "#         y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)    # to original image shape\n",
    "#         return y\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     model = CustomCNN('ResNet-18', 19)\n",
    "#     model.init_pretrained('checkpoints/backbones/resnet/resnet18.pth')\n",
    "#     x = torch.randn(2, 3, 224, 224)\n",
    "#     y = model(x)\n",
    "#     print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load pretrained model\n",
    "\n",
    "def load_model(model, model_path, save_what):   \n",
    "    if (save_what == state_dict_only):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "    if (save_what == whole_model):\n",
    "        model = torch.load(model_path)\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "def save_model(model, save_path, save_what):\n",
    "    if (save_what == state_dict_only or both):\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    if (save_what == whole_model or both):\n",
    "        torch.save(save_path)\n",
    "\n",
    "\n",
    "# model_path = '/home/zkim/outdoor_navigation/semantic-segmentation/checkpoints/pretrained/ddrnet/DDRNet23s_imagenet.pth'  # change if necessary\n",
    "# model = DDRNet().to(device) #   or some other neural network with pretrained state dict\n",
    "# model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "from torch import distributed as dist\n",
    "from semseg.models import *\n",
    "from semseg.datasets import * \n",
    "from semseg.augmentations import get_train_augmentation, get_val_augmentation\n",
    "from semseg.losses import get_loss\n",
    "from semseg.schedulers import get_scheduler\n",
    "from semseg.optimizers import get_optimizer\n",
    "from semseg.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
    "from tools.val import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mIoU = 0.0\n",
    "num_workers = 4 #mp.cpu_count() = 24 in this case\n",
    "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
    "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
    "epochs, lr = epoch, optim_cfg['LR']\n",
    "\n",
    "# transforms\n",
    "# traintransform = get_train_augmentation(train_cfg['IMAGE_SIZE'], seg_fill=dataset_cfg['IGNORE_LABEL'])\n",
    "# valtransform = get_val_augmentation(eval_cfg['IMAGE_SIZE'])\n",
    "\n",
    "trainset = cityscapes_train_dataset\n",
    "valset = cityscapes_val_dataset \n",
    "\n",
    "#img, smnt = cityscapes_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = eval(model_cfg['NAME'])(model_cfg['BACKBONE'], trainset.n_classes)\n",
    "# model.init_pretrained(model_cfg['PRETRAINED'])\n",
    "model = torchvision.models.resnet18(weights='DEFAULT') # temporary model to set to make the pipeline\n",
    "num_classes = 19\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if train_cfg['DDP']: \n",
    "    sampler = DistributedSampler(trainset, dist.get_world_size(), dist.get_rank(), shuffle=True)\n",
    "    model = DDP(model, device_ids=[gpu])\n",
    "else:\n",
    "    sampler = RandomSampler(trainset)\n",
    "\n",
    "trainloader = train_dataloader\n",
    "valloader = val_dataloader    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2975"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_per_epoch = len(trainset) // batch_size\n",
    "# class_weights = trainset.class_weights.to(device)\n",
    "loss_function = nn.CrossEntropyLoss() #get_loss(loss_cfg['NAME'], trainset.ignore_label, None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) #get_optimizer(model, optim_cfg['NAME'], lr, optim_cfg['WEIGHT_DECAY'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) #get_scheduler(sched_cfg['NAME'], optimizer, epochs * iters_per_epoch, sched_cfg['POWER'], iters_per_epoch * sched_cfg['WARMUP'], sched_cfg['WARMUP_RATIO'])\n",
    "scaler = GradScaler(enabled=train_cfg['AMP'])\n",
    "#writer = SummaryWriter(str(save_dir / 'logs'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train_model(model, trainloader, loss_fn, optimizer, scheduler, epoch, lr):\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        if train_cfg['DDP']: sampler.set_epoch(epoch)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm.tqdm(enumerate(trainloader), total=iters_per_epoch, desc=f\"Epoch: [{epoch+1}/{epochs}] Iter: [{0}/{iters_per_epoch}] LR: {lr:.8f} Loss: {train_loss:.8f}\")\n",
    "\n",
    "        for iter, (img, lbl) in pbar:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            img = img.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "            print(type(img))\n",
    "            print(type(lbl))\n",
    "            \n",
    "            with autocast(enabled=train_cfg['AMP']):\n",
    "                logits = model(img)\n",
    "                loss = loss_fn(logits, lbl)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            lr = scheduler.get_lr()\n",
    "            lr = sum(lr) / len(lr)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            pbar.set_description(f\"Epoch: [{epoch+1}/{epochs}] Iter: [{iter+1}/{iters_per_epoch}] LR: {lr:.8f} Loss: {train_loss / (iter+1):.8f}\")\n",
    "        \n",
    "        train_loss /= iter+1\n",
    "        #writer.add_scalar('train/loss', train_loss, epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (epoch+1) % train_cfg['EVAL_INTERVAL'] == 0 or (epoch+1) == epochs:\n",
    "            miou = evaluate(model, valloader, device)[-1]\n",
    "            #writer.add_scalar('val/mIoU', miou, epoch)\n",
    "\n",
    "            if miou > best_mIoU:\n",
    "                best_mIoU = miou\n",
    "                torch.save(model.module.state_dict() if train_cfg['DDP'] else model.state_dict(), save_dir / f\"{model_cfg['NAME']}_{model_cfg['BACKBONE']}_{dataset_cfg['NAME']}.pth\")\n",
    "            print(f\"Current mIoU: {miou} Best mIoU: {best_mIoU}\")\n",
    "\n",
    "    #writer.close()\n",
    "    pbar.close()\n",
    "    end = time.gmtime(time.time() - start)\n",
    "\n",
    "    table = [\n",
    "        ['Best mIoU', f\"{best_mIoU:.2f}\"],\n",
    "        ['Total Training Time', time.strftime(\"%H:%M:%S\", end)]\n",
    "    ]\n",
    "    print(tabulate(table, numalign='right'))    \n",
    "    end = time.time()\n",
    "    print(\"time taken in s: %lf\", (end - start))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1/500] Iter: [0/1487] LR: 0.00100000 Loss: 0.00000000:   0%|          | 0/1487 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 5.79 GiB of which 128.00 MiB is free. Process 346816 has 66.00 MiB memory in use. Process 348036 has 66.00 MiB memory in use. Process 350007 has 66.00 MiB memory in use. Including non-PyTorch memory, this process has 4.78 GiB memory in use. Of the allocated memory 4.67 GiB is allocated by PyTorch, and 31.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[252], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, trainloader, loss_fn, optimizer, scheduler, epoch, lr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(lbl))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39mtrain_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAMP\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 22\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, lbl)\n\u001b[1;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 5.79 GiB of which 128.00 MiB is free. Process 346816 has 66.00 MiB memory in use. Process 348036 has 66.00 MiB memory in use. Process 350007 has 66.00 MiB memory in use. Including non-PyTorch memory, this process has 4.78 GiB memory in use. Of the allocated memory 4.67 GiB is allocated by PyTorch, and 31.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_model(model, trainloader, loss_function, optimizer, scheduler, epoch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(cityscapes_val_dataset.CityscapesClass)\n",
    "#val_dataloader.dataset.classes[1][6]\n",
    "#dataloader.dataset.ignore_label\n",
    "len(val_dataloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from semseg.models import *\n",
    "from semseg.datasets import *\n",
    "from semseg.augmentations import get_val_augmentation\n",
    "from semseg.metrics import Metrics\n",
    "from semseg.utils.utils import setup_cudnn\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    #@torch.no_grad()\n",
    "    \n",
    "    print('Evaluating...')\n",
    "    model.eval()\n",
    "    #metrics = Metrics(dataloader.dataset.n_classes, dataloader.dataset.ignore_label, device)\n",
    "    metrics = Metrics(len(val_dataloader.dataset.classes), dataloader.dataset.classes[0][6], device)\n",
    "    num_classes = len(val_dataloader.dataset.classes)\n",
    "    hist = torch.zeros(num_classes, num_classes).to(device)\n",
    "    \n",
    "    for images, labels in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        preds = model(images).softmax(dim=1) \n",
    "        print('preds.size(): ', preds.size())  #torch.Size([2, 1000])\n",
    "        print('labels.size(): ', labels.size()) #torch.Size([2, 1, 256, 512])\n",
    "        #metrics.update(preds, labels)\n",
    "        keep = labels != dataloader.dataset.classes[0][6]\n",
    "        print(keep)\n",
    "        preds = preds.argmax(dim=1)\n",
    "        #keep = labels != dataloader.dataset.classes[0][6]\n",
    "        print('preds: ', preds)\n",
    "        print('preds.size(): ', preds.size())\n",
    "        print('preds[keep].size(): ', preds[keep].size())\n",
    "        print('labels[keep]: ', labels[keep])\n",
    "        print('labels[keep].size(): ', labels[keep].size())\n",
    "        hist += torch.bincount(labels[keep] * num_classes + preds[keep], minlength=num_classes**2).view(num_classes, num_classes)\n",
    "    \n",
    "    ious, miou = metrics.compute_iou()\n",
    "    acc, macc = metrics.compute_pixel_acc()\n",
    "    f1, mf1 = metrics.compute_f1()\n",
    "    \n",
    "    return acc, macc, f1, mf1, ious, miou\n",
    "\n",
    "\n",
    "#dataloader - val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds.size():  torch.Size([2, 1000])\n",
      "labels.size():  torch.Size([2, 1, 256, 512])\n",
      "tensor([[[[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]]]])\n",
      "preds:  tensor([909, 909])\n",
      "preds.size():  torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[399], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# if eval_cfg['MSF']['ENABLE']:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     acc, macc, f1, mf1, ious, miou = evaluate_msf(model, dataloader, device, eval_cfg['MSF']['SCALES'], eval_cfg['MSF']['FLIP'])\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m acc, macc, f1, mf1, ious, miou \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m table \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mCLASSES) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIoU\u001b[39m\u001b[38;5;124m'\u001b[39m: ious \u001b[38;5;241m+\u001b[39m [miou],\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1 \u001b[38;5;241m+\u001b[39m [mf1],\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m'\u001b[39m: acc \u001b[38;5;241m+\u001b[39m [macc]\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate(table, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[398], line 24\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds: \u001b[39m\u001b[38;5;124m'\u001b[39m, preds)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds.size(): \u001b[39m\u001b[38;5;124m'\u001b[39m, preds\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds[keep].size(): \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels[keep]: \u001b[39m\u001b[38;5;124m'\u001b[39m, labels[keep])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels[keep].size(): \u001b[39m\u001b[38;5;124m'\u001b[39m, labels[keep]\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# evaluation \n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "eval_cfg = cfg['EVAL']\n",
    "dataset = cityscapes_val_dataset\n",
    "dataloader = val_dataloader\n",
    "\n",
    "model_path = '' #Path(eval_cfg['MODEL_PATH'])\n",
    "# if not model_path.exists(): \n",
    "#         model_path = Path(cfg['SAVE_DIR']) / f\"{cfg['MODEL']['NAME']}_{cfg['MODEL']['BACKBONE']}_{cfg['DATASET']['NAME']}.pth\"\n",
    "print(f\"Evaluating {model_path}...\")\n",
    "\n",
    "model = resnet18() # some model\n",
    "# original - model = eval(cfg['MODEL']['NAME'])(cfg['MODEL']['BACKBONE'], dataset.n_classes)\n",
    "#model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model = model.to(device)\n",
    "\n",
    "# if eval_cfg['MSF']['ENABLE']:\n",
    "#     acc, macc, f1, mf1, ious, miou = evaluate_msf(model, dataloader, device, eval_cfg['MSF']['SCALES'], eval_cfg['MSF']['FLIP'])\n",
    "# else:\n",
    "acc, macc, f1, mf1, ious, miou = evaluate(model, dataloader, device)\n",
    "\n",
    "table = {\n",
    "    'Class': list(dataset.CLASSES) + ['Mean'],\n",
    "    'IoU': ious + [miou],\n",
    "    'F1': f1 + [mf1],\n",
    "    'Acc': acc + [macc]\n",
    "}\n",
    "\n",
    "print(tabulate(table, headers='keys'))\n",
    "\n",
    "#def test_model(model, trainloader, loss_fn, optimizer, scheduler, epoch, lr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_msf(model, dataloader, device, scales, flip): # use multiscale and flip to \n",
    "    torch.no_grad()\n",
    "    model.eval()\n",
    "\n",
    "    n_classes = len(val_dataloader.dataset.classes) #dataloader.dataset.n_classes\n",
    "    metrics = Metrics(n_classes, dataloader.dataset.ignore_label, device)\n",
    "\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        labels = labels.to(device)\n",
    "        B, H, W = labels.shape\n",
    "        scaled_logits = torch.zeros(B, n_classes, H, W).to(device)\n",
    "\n",
    "        for scale in scales:\n",
    "            new_H, new_W = int(scale * H), int(scale * W)\n",
    "            new_H, new_W = int(math.ceil(new_H / 32)) * 32, int(math.ceil(new_W / 32)) * 32\n",
    "            scaled_images = F.interpolate(images, size=(new_H, new_W), mode='bilinear', align_corners=True)\n",
    "            scaled_images = scaled_images.to(device)\n",
    "            logits = model(scaled_images)\n",
    "            logits = F.interpolate(logits, size=(H, W), mode='bilinear', align_corners=True)\n",
    "            scaled_logits += logits.softmax(dim=1)\n",
    "\n",
    "            if flip:\n",
    "                scaled_images = torch.flip(scaled_images, dims=(3,))\n",
    "                logits = model(scaled_images)\n",
    "                logits = torch.flip(logits, dims=(3,))\n",
    "                logits = F.interpolate(logits, size=(H, W), mode='bilinear', align_corners=True)\n",
    "                scaled_logits += logits.softmax(dim=1)\n",
    "\n",
    "        metrics.update(scaled_logits, labels)\n",
    "    \n",
    "    acc, macc = metrics.compute_pixel_acc()\n",
    "    f1, mf1 = metrics.compute_f1()\n",
    "    ious, miou = metrics.compute_iou()\n",
    "    return acc, macc, f1, mf1, ious, miou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
